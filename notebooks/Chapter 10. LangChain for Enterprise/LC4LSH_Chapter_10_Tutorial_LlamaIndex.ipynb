{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Code to Chapter 10 of LangChain for Life Science and Healthcare book, by Dr. Ivan Reznikov\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NVOYHL7FBScEdej2SLbUJuzUgCJ62usO?usp=sharing)\n",
    "\n",
    "## LlamaIndex Tutorial - Complete Guide\n",
    "\n",
    "## Introduction to LlamaIndex\n",
    "\n",
    "LlamaIndex is a powerful data framework designed to connect Large Language Models (LLMs) with external data sources. It provides tools for indexing, storing, and querying documents using vector embeddings, making it easy to build Retrieval-Augmented Generation (RAG) applications.\n",
    "\n",
    "**Key Features:**\n",
    "- **Document Loading**: Support for various file formats (PDF, text, etc.)\n",
    "- **Vector Indexing**: Creates searchable embeddings from your documents\n",
    "- **Query Engine**: Natural language querying of your data\n",
    "- **Customizable**: Flexible LLM and prompt customization\n",
    "- **Persistent Storage**: Save and load indexes for reuse"
   ],
   "metadata": {
    "id": "pGyvVFDofj7J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install the required packages. We're using specific versions to ensure compatibility."
   ],
   "metadata": {
    "id": "jSonMzAFTnFr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5jlMQblkT6a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "64fd7a9f-d9c4-4dca-bfdc-637749f97034"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#!pip install -qU llama-index==0.12.10 openai==1.59.7\n",
    "!pip install -qU llama-index openai"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip freeze | grep \"llama\\|openai\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6MNBKW3i8PT",
    "outputId": "f5f10810-4216-4fc8-83e6-c942b35876a6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "llama-cloud==0.1.8\n",
      "llama-index==0.12.10\n",
      "llama-index-agent-openai==0.4.1\n",
      "llama-index-cli==0.4.0\n",
      "llama-index-core==0.12.10.post1\n",
      "llama-index-embeddings-huggingface==0.5.0\n",
      "llama-index-embeddings-openai==0.3.1\n",
      "llama-index-indices-managed-llama-cloud==0.6.3\n",
      "llama-index-llms-openai==0.3.13\n",
      "llama-index-multi-modal-llms-openai==0.4.2\n",
      "llama-index-program-openai==0.3.1\n",
      "llama-index-question-gen-openai==0.3.0\n",
      "llama-index-readers-file==0.4.3\n",
      "llama-index-readers-llama-parse==0.4.0\n",
      "llama-parse==0.5.19\n",
      "openai==1.59.7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. API Key Configuration\n",
    "\n",
    "Setting up OpenAI API access for embeddings and LLM functionality.\n",
    "\n",
    "**Important Notes:**\n",
    "- Replace `userdata.get(\"LC4LS_OPENAI_API_KEY\")` with your actual API key if not using Colab\n",
    "- Keep your API key secure and never commit it to version control\n",
    "- Alternative: Use `os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"`"
   ],
   "metadata": {
    "id": "6Kh5WsQYTwN_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"LC4LS_OPENAI_API_KEY\")"
   ],
   "metadata": {
    "id": "sbQojwXrcmHi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Document Preparation\n",
    "\n",
    "Creating a directory structure and downloading a sample PDF document.\n",
    "\n",
    "**What's happening here:**\n",
    "- We're downloading a research paper about watermarking protein generative models\n",
    "- The headers help avoid blocking by the server\n",
    "- The PDF is saved locally for processing"
   ],
   "metadata": {
    "id": "yn2dHtD9T4vP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "os.makedirs('./data', exist_ok=True)"
   ],
   "metadata": {
    "id": "X8vMy8ITYHpU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "    'Referer': 'https://github.com/IvanReznikov/LangChain4LifeScience/blob/main/data/articles/2410.20354v4.pdf',\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    'https://raw.githubusercontent.com/IvanReznikov/LangChain4LifeScience/refs/heads/main/data/articles/2410.20354v4.pdf',\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "pdf_path = \"./data/article.pdf\"\n",
    "with open(pdf_path, \"wb\") as f:\n",
    "    f.write(response.content)"
   ],
   "metadata": {
    "id": "sFB7QEg3VsIf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Document Loading with PDFReader\n",
    "\n",
    "LlamaIndex provides specialized loaders for different file types.\n",
    "\n",
    "**Key Points:**\n",
    "- `PDFReader` extracts text content from PDF files\n",
    "- Documents are converted into LlamaIndex's internal format\n",
    "- Each page typically becomes a separate document object"
   ],
   "metadata": {
    "id": "_KHdD8iBT-C0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import download_loader\n",
    "\n",
    "PDFReader = download_loader(\"PDFReader\")\n",
    "\n",
    "loader = PDFReader()\n",
    "documents = loader.load_data(file=pdf_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQguypLRWTA_",
    "outputId": "0b9a92f8-bc50-40d4-c442-8cd6ac65af46"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-6-668be708eb6c>:3: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  PDFReader = download_loader(\"PDFReader\")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Creating Vector Index and Query Engine\n",
    "\n",
    "This is the core of RAG - converting documents into searchable vectors.\n",
    "\n",
    "**Technical Details:**\n",
    "- **Vector Index**: Creates numerical representations (embeddings) of document chunks\n",
    "- **text-embedding-3-large**: OpenAI's most capable embedding model\n",
    "- **Query Engine**: Handles similarity search and response generation"
   ],
   "metadata": {
    "id": "jKYpHWdHUC1D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(model = \"text-embedding-3-large\")\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "query_engine = index.as_query_engine()"
   ],
   "metadata": {
    "id": "oYXrqWY8kbjX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected Behavior:**\n",
    "- The system will find relevant document sections about protein watermarking\n",
    "- Generate a comprehensive answer based on the paper's content\n",
    "- Response should mention benefits like intellectual property protection, model verification, etc."
   ],
   "metadata": {
    "id": "YVxU-vhlUJE4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = query_engine.query(\"What are the benefits of watermarking protein generative models?\")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_P-4avBb1mn",
    "outputId": "617b8779-9959-4374-83a4-e13312f6d0ff"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The benefits of watermarking protein generative models include copyright authentication, tracking of generated structures, protection against unauthorized use, and the ability to identify the rightful owner of the model or generated structures.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Saving and Loading Index\n",
    "\n",
    "To avoid rebuilding the index every time, we can persist it to disk.\n",
    "\n",
    "**Benefits of Persistence:**\n",
    "- Saves time on subsequent runs\n",
    "- Preserves expensive embedding computations\n",
    "- Enables sharing indexes between sessions"
   ],
   "metadata": {
    "id": "SYCicUypfL5f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "index.storage_context.persist(\"_index\")"
   ],
   "metadata": {
    "id": "yDLZsNied6ed"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading a Saved Index"
   ],
   "metadata": {
    "id": "NYzQuU1sUZ_r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"_index\")\n",
    "\n",
    "# load index\n",
    "new_index = load_index_from_storage(storage_context, embed_model=embed_model)"
   ],
   "metadata": {
    "id": "3ZKWedL0eaHE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important Notes:**\n",
    "- Must use the same embedding model when loading\n",
    "- The `persist_dir` should match the save location\n",
    "- Loaded index should provide identical results"
   ],
   "metadata": {
    "id": "z00CiFJGUdaC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "new_query_engine = new_index.as_query_engine()\n",
    "response = new_query_engine.query(\"What are the benefits of watermarking protein generative models?\")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dRCZNgieqCH",
    "outputId": "27bf08d3-1009-41a7-f9eb-50fe428f1437"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The benefits of watermarking protein generative models include copyright authentication, tracking of generated structures, protection against unauthorized use, and the ability to prove ownership of artificially generated structures.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Customizing LLMs\n",
    "\n",
    "LlamaIndex allows you to customize the language model used for generating responses.\n",
    "\n",
    "**Configuration Options:**\n",
    "- **temperature=0**: Makes responses more deterministic and consistent\n",
    "- **model_name=\"gpt-4o-mini\"**: Uses a specific GPT model variant\n",
    "- **chat_mode=\"context\"**: Maintains conversation context\n",
    "\n",
    "**Chat Engine vs Query Engine:**\n",
    "- **Query Engine**: Stateless, each query is independent\n",
    "- **Chat Engine**: Maintains conversation history and context"
   ],
   "metadata": {
    "id": "ylgvtjYYhUsR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#from langchain.chat_models import ChatOpenAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm=OpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"context\", llm=llm)"
   ],
   "metadata": {
    "id": "VbBjscd0gJvb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response = chat_engine.chat(\"What are the benefits of watermarking protein generative models?\")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiEjCpzfgohI",
    "outputId": "3685c220-3454-4087-a8b2-73bbacd4a421"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Watermarking protein generative models offers several benefits, including:\n",
      "\n",
      "1. **Copyright Authentication**: Watermarking allows for the authentication of protein structures generated by the model, helping to protect the intellectual property rights of the original creators.\n",
      "\n",
      "2. **Tracking Generated Structures**: Watermarking enables the tracking of generated protein structures, which can be useful for auditing the use of protein generative models and identifying the original source of the structures.\n",
      "\n",
      "3. **User-Specific Information Embedding**: Watermarking can embed user-specific information into protein structures, allowing for personalized identification and ownership verification.\n",
      "\n",
      "4. **Negligible Impact on Structure Quality**: The watermarking framework has been designed to have a negligible impact on the original protein structure quality, ensuring that the integrity and accuracy of the generated structures are maintained.\n",
      "\n",
      "5. **Robustness**: The watermarking method is robust against potential post-processing and adaptive attacks, enhancing the security and reliability of the protein generative models.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Custom Prompt Engineering\n",
    "\n",
    "Fine-tune how the system responds by customizing prompts.\n",
    "\n",
    "**Prompt Template Variables:**\n",
    "- `{context_str}`: Replaced with relevant document chunks\n",
    "- `{query_str}`: Replaced with the user's question\n",
    "\n",
    "**Expected Result:**\n",
    "- Every response should now start with \"According to the article\"\n",
    "- This ensures clear attribution to the source document"
   ],
   "metadata": {
    "id": "xSyCbMk-kM3d"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import Prompt\n",
    "\n",
    "template = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the question and each answer should start with 'According to the article': {query_str}\\n\"\n",
    ")\n",
    "qa_template = Prompt(template)"
   ],
   "metadata": {
    "id": "V4ciXrGYg1Fs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query_engine = index.as_query_engine(text_qa_template=qa_template)\n",
    "response = query_engine.query(\"What are the benefits of watermarking protein generative models?\")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0truEw_h2lm",
    "outputId": "ddbe2dae-0658-4d64-ede2-804f33b971d8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "According to the article, the benefits of watermarking protein generative models include:\n",
      "1. Providing copyright authentication for generated protein structures.\n",
      "2. Allowing tracking of generated structures to prevent unauthorized use.\n",
      "3. Ensuring user-specific information can be embedded in protein structures.\n",
      "4. Preserving generation quality while learning to generate watermarked structures.\n",
      "5. Exerting a negligible impact on the original protein structure quality.\n",
      "6. Being robust under potential post-processing and adaptive attacks.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Jp6v3tuTUr72"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
