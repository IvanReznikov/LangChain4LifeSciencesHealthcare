{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Code to Chapter 10 of LangChain for Life Science and Healthcare book, by Dr. Ivan Reznikov\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fqilNVeTErmrpkZ-qVEUzc54zCnIYWGy?usp=sharing)\n",
        "\n",
        "## RAG Tutorial with AdalFlow\n",
        "\n",
        "This notebook demonstrates how to build a complete Retrieval-Augmented Generation (RAG) system using the AdalFlow library. We'll walk through setting up the environment, processing documents, creating embeddings, and implementing a full RAG pipeline.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Environment Setup](#environment-setup)\n",
        "2. [Configuration](#configuration)\n",
        "3. [Data Pipeline Creation](#data-pipeline-creation)\n",
        "4. [Document Processing](#document-processing)\n",
        "5. [Database Setup](#database-setup)\n",
        "6. [RAG Pipeline Implementation](#rag-pipeline-implementation)\n",
        "7. [Testing and Results](#testing-and-results)"
      ],
      "metadata": {
        "id": "rHA6d73J8wcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "\n",
        "First, we need to install the required dependencies for our RAG system:"
      ],
      "metadata": {
        "id": "ln1YAzmQ9BqW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "THTvmhjgfiHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6762f5e2-c259-49c5-8cfb-1a31a6d086ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.1/310.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU adalflow[openai] PyPDF2 pyvis faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# patch for colab to run\n",
        "\n",
        "!pip -q uninstall httpx anyio -y\n",
        "!pip -q install \"anyio>=3.1.0,<4.0\"\n",
        "!pip -q install httpx==0.24.1"
      ],
      "metadata": {
        "id": "RQZJtZmgqPwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371b2962-acc4-4a38-8e62-42842597a89d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/80.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.27.0 requires httpx<1.0.0,>=0.28.1, which is not installed.\n",
            "openai 1.97.1 requires httpx<1,>=0.23.0, which is not installed.\n",
            "gradio 5.38.1 requires httpx<1.0,>=0.24.1, which is not installed.\n",
            "google-genai 1.27.0 requires anyio<5.0.0,>=4.8.0, but you have anyio 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.27.0 requires anyio<5.0.0,>=4.8.0, but you have anyio 3.7.1 which is incompatible.\n",
            "google-genai 1.27.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.24.1 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep \"adal\\|httpx\\|faiss\\|openai\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38H5LCoO6Mo3",
        "outputId": "879f6f39-8383-4fe4-e74e-36eff08c5c45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adalflow==1.0.4\n",
            "faiss-cpu==1.11.0.post1\n",
            "httpx==0.24.1\n",
            "openai==1.97.1\n",
            "safehttpx==0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"LC4LS_OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "ONfzF9Puzdd_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Architecture Overview\n",
        "\n",
        "Unlike other libraries, AdalFlow's RAG pipeline consists of two main components:\n",
        "\n",
        "1. **Task Pipeline**: Contains a retriever and a generator for query processing\n",
        "2. **Data Pipeline**: Handles preprocessing and persistence of documents with local/cloud databases\n",
        "\n",
        "This architecture mirrors real production environments where data processing and query handling are separated for better scalability and maintainability."
      ],
      "metadata": {
        "id": "aVOZV19irzpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "We centralize all system configurations in a single dictionary for easy management:"
      ],
      "metadata": {
        "id": "WrUmNr8dVh1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs = {\n",
        "    \"embedder\": {\n",
        "        \"batch_size\": 100,\n",
        "        \"model_kwargs\": {\n",
        "            \"model\": \"text-embedding-3-large\",\n",
        "            \"dimensions\": 1024,\n",
        "            \"encoding_format\": \"float\",\n",
        "        },\n",
        "    },\n",
        "    \"retriever\": {\n",
        "        \"top_k\": 2,\n",
        "    },\n",
        "    \"generator\": {\n",
        "        \"model\": \"gpt-4o-mini\",\n",
        "        \"temperature\": 0.1,\n",
        "        \"stream\": False,\n",
        "    },\n",
        "    \"text_splitter\": {\n",
        "        \"split_by\": \"word\",\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "G2Kpc4d6Vq2H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pipeline Creation\n",
        "\n",
        "We will use local data base `LocalDB` and `core.data_process` to create a data processing pipeline. This data pipeline will split documents into chunks and work with `LocalDB` to persis the transformed/processed documents in local file `index.faiss` (pickle format).\n",
        "\n",
        "Data pipeline requires a sequence of `Document` as inputs."
      ],
      "metadata": {
        "id": "vu1WsDy0Vtdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adalflow.components.data_process import (\n",
        "    RetrieverOutputToContextStr,\n",
        "    ToEmbeddings,\n",
        "    TextSplitter,\n",
        ")\n",
        "from adalflow.core import Embedder, Sequential, Component, Generator, ModelClient\n",
        "from adalflow.core.types import Document, ModelClientType\n",
        "\n",
        "\n",
        "def prepare_data_pipeline():\n",
        "    splitter = TextSplitter(**configs[\"text_splitter\"])\n",
        "    embedder = Embedder(\n",
        "        model_client=ModelClientType.OPENAI(),\n",
        "        model_kwargs=configs[\"embedder\"][\"model_kwargs\"],\n",
        "    )\n",
        "    embedder_transformer = ToEmbeddings(\n",
        "        embedder=embedder, batch_size=configs[\"embedder\"][\"batch_size\"]\n",
        "    )\n",
        "    data_transformer = Sequential(splitter, embedder_transformer)\n",
        "    return data_transformer"
      ],
      "metadata": {
        "id": "aOklnBPqVxb_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformer = prepare_data_pipeline()\n",
        "data_transformer"
      ],
      "metadata": {
        "id": "E24GznkJV1-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ed298c-333f-404e-d8da-155c7aabd2d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): TextSplitter(split_by=word, chunk_size=500, chunk_overlap=100)\n",
              "  (1): ToEmbeddings(\n",
              "    batch_size=100\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "    (batch_embedder): BatchEmbedder(\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Processing\n",
        "\n",
        "Now we'll download a research paper and convert it into processable documents:\n",
        "\n",
        "### Download Research Paper"
      ],
      "metadata": {
        "id": "JrVUIxen-Lgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./data', exist_ok=True)"
      ],
      "metadata": {
        "id": "-t0mlJwt9Hgs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
        "    'Referer': 'https://github.com/IvanReznikov/LangChain4LifeScience/blob/main/data/articles/2410.20354v4.pdf',\n",
        "}\n",
        "\n",
        "response = requests.get(\n",
        "    'https://raw.githubusercontent.com/IvanReznikov/LangChain4LifeScience/refs/heads/main/data/articles/2410.20354v4.pdf',\n",
        "    headers=headers,\n",
        ")\n",
        "\n",
        "pdf_path = \"./data/article.pdf\"\n",
        "with open(pdf_path, \"wb\") as f:\n",
        "    f.write(response.content)"
      ],
      "metadata": {
        "id": "SoYFukAz0wbQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF Text Extraction\n"
      ],
      "metadata": {
        "id": "D3esmP4f-Qxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from uuid import uuid4\n",
        "\n",
        "def pdf_to_documents(pdf_path):\n",
        "    # Read the PDF file\n",
        "    reader = PdfReader(pdf_path)\n",
        "    documents = []\n",
        "\n",
        "    # Loop through each page to extract text\n",
        "    for i, page in enumerate(reader.pages):\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            # Create a dictionary for each page\n",
        "            doc = {\n",
        "                \"meta_data\": {\"title\": f\"Page {i + 1} of {pdf_path.split('/')[-1]}\"},\n",
        "                \"text\": text,\n",
        "                \"id\": f\"doc{i + 1}\"\n",
        "            }\n",
        "            documents.append(Document(*doc))\n",
        "\n",
        "    return documents\n",
        "\n",
        "docs = pdf_to_documents(pdf_path)"
      ],
      "metadata": {
        "id": "G9L7-HdluEIt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform Documents\n",
        "\n",
        "Apply the data pipeline to convert documents into embeddings:"
      ],
      "metadata": {
        "id": "tgd9qYJx-TXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_documents = data_transformer(docs)"
      ],
      "metadata": {
        "id": "LBy8oIkLWtHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222c0187-fb11-4ad0-e6cc-0ab874355549"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting Documents in Batches: 100%|██████████| 1/1 [00:00<00:00, 1336.62it/s]\n",
            "Batch embedding documents: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n",
            "Adding embeddings to documents from batch: 1it [00:00, 2721.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Database Setup\n",
        "\n",
        "We will use localdb to manage the `documents`, `transformers`, and the persistance of the transformed documents. This resembles more of the production environment where the embeddings and documents are often handled in data base and can be reused to save cost."
      ],
      "metadata": {
        "id": "uKSZmRxeX5t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./index', exist_ok=True)\n",
        "index_name = \"index.faiss\"\n",
        "index_key = \"data_transformer\""
      ],
      "metadata": {
        "id": "rpsmCaNyBp-u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import os\n",
        "from adalflow.core.db import LocalDB\n",
        "\n",
        "def prepare_database_with_index(\n",
        "    docs: List[Document],\n",
        "    index_key: str = \"data_transformer\",\n",
        "    index_path: str = \"./index/default\"\n",
        "):\n",
        "    if os.path.exists(index_path):\n",
        "        os.remove(index_path)\n",
        "\n",
        "    db = LocalDB()\n",
        "    db.load(docs)\n",
        "\n",
        "    data_transformer = prepare_data_pipeline()\n",
        "\n",
        "    # 🧩 Fix: manually register then apply\n",
        "    key = db.register_transformer(transformer=data_transformer, key=index_key)\n",
        "    db.transform(key=key)\n",
        "\n",
        "    db.save_state(index_path)\n",
        "    print(db)\n"
      ],
      "metadata": {
        "id": "fhVKVN7MYVVZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the database for retriever\n",
        "\n",
        "prepare_database_with_index(docs, index_key=index_key, index_path=f\"./index/{index_name}\")"
      ],
      "metadata": {
        "id": "fviYAerVYjL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd4ea56-d4ee-45ad-88f2-94f51439adb6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting Documents in Batches: 100%|██████████| 1/1 [00:00<00:00, 1298.14it/s]\n",
            "Batch embedding documents: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
            "Adding embeddings to documents from batch: 1it [00:00, 6345.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved the state of the DB to ./index/index.faiss\n",
            "LocalDB(name='LocalDB', items=[Document(id=830f3afb-2243-4256-a377-5c98ef000042, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=fb77bee6-1585-4ca5-900e-fd89c826990b, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=e67bb650-51b8-401d-babe-751a8ecb30b2, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=174f8ae9-24e4-4f8c-979e-042b29c80a22, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=2b51fdd1-4bf4-4ee1-a353-9a615ffa6d6d, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=f11d6778-8c79-4bfc-9b61-c1f7273c472f, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=cb4e0c8b-3b6e-4f8b-a8c1-b738097abb15, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=62f7dc1d-0be8-49cf-b86d-4d962a8db4b3, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=078196ae-680a-41dd-b579-0d581a323f87, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=9203b215-07ce-4c54-9d99-8b31e67130a7, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=98aac68c-9cba-46a8-b60b-7f063b2df3d6, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=830f3afb-2243-4256-a377-5c98ef000042, order=0, score=None), Document(id=da8d6901-3ebc-4751-839e-87ec1fc15598, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=fb77bee6-1585-4ca5-900e-fd89c826990b, order=0, score=None), Document(id=3f663a0d-4492-4917-938d-a6330d3bca75, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=e67bb650-51b8-401d-babe-751a8ecb30b2, order=0, score=None), Document(id=98bae3a1-a8d1-40e8-a30f-75e6cec5d445, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=174f8ae9-24e4-4f8c-979e-042b29c80a22, order=0, score=None), Document(id=a0eb4dd9-6e3b-4f9f-8c5c-1797f01426ac, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=2b51fdd1-4bf4-4ee1-a353-9a615ffa6d6d, order=0, score=None), Document(id=eae7e279-7b10-42bf-9489-56a3deaf4595, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=f11d6778-8c79-4bfc-9b61-c1f7273c472f, order=0, score=None), Document(id=f0ed9f18-d024-4cec-b5e1-5f0156e75a88, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=cb4e0c8b-3b6e-4f8b-a8c1-b738097abb15, order=0, score=None), Document(id=f0181e2b-9180-43ad-99f4-420ace59b829, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=62f7dc1d-0be8-49cf-b86d-4d962a8db4b3, order=0, score=None), Document(id=f6fd734e-51bc-4680-b6ce-c7f70d956756, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=078196ae-680a-41dd-b579-0d581a323f87, order=0, score=None), Document(id=f47b04f6-58a0-4176-aba3-a28c58e387a4, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=9203b215-07ce-4c54-9d99-8b31e67130a7, order=0, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
            "  (0): TextSplitter(split_by=word, chunk_size=500, chunk_overlap=100)\n",
            "  (1): ToEmbeddings(\n",
            "    batch_size=100\n",
            "    (embedder): Embedder(\n",
            "      model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
            "      (model_client): OpenAIClient()\n",
            "    )\n",
            "    (batch_embedder): BatchEmbedder(\n",
            "      (embedder): Embedder(\n",
            "        model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
            "        (model_client): OpenAIClient()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")}, mapper_setups={}, index_path='./index/index.faiss')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading from Persistent Storage\n",
        "\n",
        "LocalDB `save_state` not only persist the transformed documents, but also the `data_transformer`.\n",
        "\n",
        "This is really helpful as your retriever needs to have a matching `embedder` to embed the string query. Saving the transformer lets you verify and know what embedder you need to pass to Retriever."
      ],
      "metadata": {
        "id": "a-97wL5gZZKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = LocalDB.load_state(f\"./index/{index_name}\")\n",
        "db"
      ],
      "metadata": {
        "id": "cmKZnA-YZNa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f5914e-38ae-46a8-dad9-35a4a9135234"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LocalDB(name='LocalDB', items=[Document(id=830f3afb-2243-4256-a377-5c98ef000042, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=fb77bee6-1585-4ca5-900e-fd89c826990b, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=e67bb650-51b8-401d-babe-751a8ecb30b2, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=174f8ae9-24e4-4f8c-979e-042b29c80a22, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=2b51fdd1-4bf4-4ee1-a353-9a615ffa6d6d, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=f11d6778-8c79-4bfc-9b61-c1f7273c472f, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=cb4e0c8b-3b6e-4f8b-a8c1-b738097abb15, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=62f7dc1d-0be8-49cf-b86d-4d962a8db4b3, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=078196ae-680a-41dd-b579-0d581a323f87, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=9203b215-07ce-4c54-9d99-8b31e67130a7, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=98aac68c-9cba-46a8-b60b-7f063b2df3d6, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=830f3afb-2243-4256-a377-5c98ef000042, order=0, score=None), Document(id=da8d6901-3ebc-4751-839e-87ec1fc15598, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=fb77bee6-1585-4ca5-900e-fd89c826990b, order=0, score=None), Document(id=3f663a0d-4492-4917-938d-a6330d3bca75, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=e67bb650-51b8-401d-babe-751a8ecb30b2, order=0, score=None), Document(id=98bae3a1-a8d1-40e8-a30f-75e6cec5d445, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=174f8ae9-24e4-4f8c-979e-042b29c80a22, order=0, score=None), Document(id=a0eb4dd9-6e3b-4f9f-8c5c-1797f01426ac, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=2b51fdd1-4bf4-4ee1-a353-9a615ffa6d6d, order=0, score=None), Document(id=eae7e279-7b10-42bf-9489-56a3deaf4595, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=f11d6778-8c79-4bfc-9b61-c1f7273c472f, order=0, score=None), Document(id=f0ed9f18-d024-4cec-b5e1-5f0156e75a88, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=cb4e0c8b-3b6e-4f8b-a8c1-b738097abb15, order=0, score=None), Document(id=f0181e2b-9180-43ad-99f4-420ace59b829, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=62f7dc1d-0be8-49cf-b86d-4d962a8db4b3, order=0, score=None), Document(id=f6fd734e-51bc-4680-b6ce-c7f70d956756, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=078196ae-680a-41dd-b579-0d581a323f87, order=0, score=None), Document(id=f47b04f6-58a0-4176-aba3-a28c58e387a4, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=9203b215-07ce-4c54-9d99-8b31e67130a7, order=0, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
              "  (0): TextSplitter(split_by=word, chunk_size=500, chunk_overlap=100)\n",
              "  (1): ToEmbeddings(\n",
              "    batch_size=100\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "    (batch_embedder): BatchEmbedder(\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")}, mapper_setups={}, index_path='./index/index.faiss')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.get_transformed_data(key=index_key)"
      ],
      "metadata": {
        "id": "ymjXtUQ3axjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4681843f-639e-4f03-f89c-7ee711e93240"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id=98aac68c-9cba-46a8-b60b-7f063b2df3d6, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=830f3afb-2243-4256-a377-5c98ef000042, order=0, score=None),\n",
              " Document(id=da8d6901-3ebc-4751-839e-87ec1fc15598, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=fb77bee6-1585-4ca5-900e-fd89c826990b, order=0, score=None),\n",
              " Document(id=3f663a0d-4492-4917-938d-a6330d3bca75, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=e67bb650-51b8-401d-babe-751a8ecb30b2, order=0, score=None),\n",
              " Document(id=98bae3a1-a8d1-40e8-a30f-75e6cec5d445, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=174f8ae9-24e4-4f8c-979e-042b29c80a22, order=0, score=None),\n",
              " Document(id=a0eb4dd9-6e3b-4f9f-8c5c-1797f01426ac, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=2b51fdd1-4bf4-4ee1-a353-9a615ffa6d6d, order=0, score=None),\n",
              " Document(id=eae7e279-7b10-42bf-9489-56a3deaf4595, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=f11d6778-8c79-4bfc-9b61-c1f7273c472f, order=0, score=None),\n",
              " Document(id=f0ed9f18-d024-4cec-b5e1-5f0156e75a88, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=cb4e0c8b-3b6e-4f8b-a8c1-b738097abb15, order=0, score=None),\n",
              " Document(id=f0181e2b-9180-43ad-99f4-420ace59b829, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=62f7dc1d-0be8-49cf-b86d-4d962a8db4b3, order=0, score=None),\n",
              " Document(id=f6fd734e-51bc-4680-b6ce-c7f70d956756, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=078196ae-680a-41dd-b579-0d581a323f87, order=0, score=None),\n",
              " Document(id=f47b04f6-58a0-4176-aba3-a28c58e387a4, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=9203b215-07ce-4c54-9d99-8b31e67130a7, order=0, score=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipeline Implementation\n",
        "\n",
        "Now we'll create the complete RAG system that combines retrieval and generation:\n",
        "\n",
        "* db (we will load from index_path), we will use `data_transformer` as the key to load the transformed documents.\n",
        "* `FAISSRetriever` which will use embeddings to perform semantic search, and return similarity score in range [0, 1].\n",
        "* `RetrieverOutputToContextStr`: this will convert the retrieved documents to a single str.\n",
        "* `Generator`: we will use a simple `JsonParser` to output a dict with field `answer`."
      ],
      "metadata": {
        "id": "Ve655cL3Y1cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Any, List\n",
        "import os\n",
        "\n",
        "from adalflow.core.db import LocalDB\n",
        "from adalflow.core.component import Component\n",
        "\n",
        "from adalflow.components.retriever.faiss_retriever import FAISSRetriever\n",
        "from adalflow.components.model_client.openai_client import OpenAIClient\n",
        "from adalflow.core.string_parser import JsonParser\n",
        "\n",
        "\n",
        "\n",
        "rag_prompt_task_desc = r\"\"\"\n",
        "You are a helpful assistant.\n",
        "\n",
        "Your task is to answer the query that may or may not come with context information.\n",
        "When context is provided, you should stick to the context and less on your prior knowledge to answer the query.\n",
        "\n",
        "Output JSON format:\n",
        "{\n",
        "    \"answer\": \"The answer to the query\",\n",
        "}\"\"\"\n",
        "\n",
        "\n",
        "class RAG(Component):\n",
        "    def __init__(self, index_path: str = f\"./index/{index_name}\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.db = LocalDB.load_state(index_path)\n",
        "\n",
        "        # ✅ FIXED: Access transformed data using dict directly\n",
        "        self.transformed_docs: List[Document] = self.db.transformed_items[index_key]\n",
        "\n",
        "        embedder = Embedder(\n",
        "            model_client=ModelClientType.OPENAI(),\n",
        "            model_kwargs=configs[\"embedder\"][\"model_kwargs\"],\n",
        "        )\n",
        "\n",
        "        self.retriever = FAISSRetriever(\n",
        "            **configs[\"retriever\"],\n",
        "            embedder=embedder,\n",
        "            documents=self.transformed_docs,\n",
        "            document_map_func=lambda doc: doc.vector,\n",
        "        )\n",
        "\n",
        "        self.retriever_output_processors = RetrieverOutputToContextStr(deduplicate=True)\n",
        "\n",
        "        self.generator = Generator(\n",
        "            prompt_kwargs={\n",
        "                \"task_desc_str\": rag_prompt_task_desc,\n",
        "            },\n",
        "            model_client=OpenAIClient(),\n",
        "            model_kwargs=configs[\"generator\"],\n",
        "            output_processors=JsonParser(),\n",
        "        )\n",
        "\n",
        "    def generate(self, query: str, context: Optional[str] = None) -> Any:\n",
        "        if not self.generator:\n",
        "            raise ValueError(\"Generator is not set\")\n",
        "\n",
        "        prompt_kwargs = {\n",
        "            \"context_str\": context,\n",
        "            \"input_str\": query,\n",
        "        }\n",
        "        response = self.generator(prompt_kwargs=prompt_kwargs)\n",
        "        return response\n",
        "\n",
        "    def call(self, query: str) -> Any:\n",
        "        retrieved_documents = self.retriever(query)\n",
        "\n",
        "        # 🧩 Re-attach original documents\n",
        "        for i, retriever_output in enumerate(retrieved_documents):\n",
        "            retrieved_documents[i].documents = [\n",
        "                self.transformed_docs[doc_index]\n",
        "                for doc_index in retriever_output.doc_indices\n",
        "            ]\n",
        "\n",
        "        print(f\"retrieved_documents: \\n {retrieved_documents}\\n\")\n",
        "\n",
        "        context_str = self.retriever_output_processors(retrieved_documents)\n",
        "        print(f\"context_str: \\n {context_str}\\n\")\n",
        "\n",
        "        return self.generate(query, context=context_str), retrieved_documents"
      ],
      "metadata": {
        "id": "LdoWsK6dbr0L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize rag and visualize its structure\n",
        "rag = RAG(index_path=f\"./index/{index_name}\")\n",
        "rag"
      ],
      "metadata": {
        "id": "tgFlKDDTcEEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299f3198-faf4-4215-ee3b-214757abf314"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RAG(\n",
              "  (db): LocalDB(name='LocalDB', items=[Document(id=830f3afb-2243-4256-a377-5c98ef000042, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=fb77bee6-1585-4ca5-900e-fd89c826990b, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=e67bb650-51b8-401d-babe-751a8ecb30b2, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=174f8ae9-24e4-4f8c-979e-042b29c80a22, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=2b51fdd1-4bf4-4ee1-a353-9a615ffa6d6d, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=f11d6778-8c79-4bfc-9b61-c1f7273c472f, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=cb4e0c8b-3b6e-4f8b-a8c1-b738097abb15, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=62f7dc1d-0be8-49cf-b86d-4d962a8db4b3, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=078196ae-680a-41dd-b579-0d581a323f87, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None), Document(id=9203b215-07ce-4c54-9d99-8b31e67130a7, text='meta_data', meta_data=text, vector='len: 2', parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=98aac68c-9cba-46a8-b60b-7f063b2df3d6, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=830f3afb-2243-4256-a377-5c98ef000042, order=0, score=None), Document(id=da8d6901-3ebc-4751-839e-87ec1fc15598, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=fb77bee6-1585-4ca5-900e-fd89c826990b, order=0, score=None), Document(id=3f663a0d-4492-4917-938d-a6330d3bca75, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=e67bb650-51b8-401d-babe-751a8ecb30b2, order=0, score=None), Document(id=98bae3a1-a8d1-40e8-a30f-75e6cec5d445, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=174f8ae9-24e4-4f8c-979e-042b29c80a22, order=0, score=None), Document(id=a0eb4dd9-6e3b-4f9f-8c5c-1797f01426ac, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=2b51fdd1-4bf4-4ee1-a353-9a615ffa6d6d, order=0, score=None), Document(id=eae7e279-7b10-42bf-9489-56a3deaf4595, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=f11d6778-8c79-4bfc-9b61-c1f7273c472f, order=0, score=None), Document(id=f0ed9f18-d024-4cec-b5e1-5f0156e75a88, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=cb4e0c8b-3b6e-4f8b-a8c1-b738097abb15, order=0, score=None), Document(id=f0181e2b-9180-43ad-99f4-420ace59b829, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=62f7dc1d-0be8-49cf-b86d-4d962a8db4b3, order=0, score=None), Document(id=f6fd734e-51bc-4680-b6ce-c7f70d956756, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=078196ae-680a-41dd-b579-0d581a323f87, order=0, score=None), Document(id=f47b04f6-58a0-4176-aba3-a28c58e387a4, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=9203b215-07ce-4c54-9d99-8b31e67130a7, order=0, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
              "    (0): TextSplitter(split_by=word, chunk_size=500, chunk_overlap=100)\n",
              "    (1): ToEmbeddings(\n",
              "      batch_size=100\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "      (batch_embedder): BatchEmbedder(\n",
              "        (embedder): Embedder(\n",
              "          model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
              "          (model_client): OpenAIClient()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )}, mapper_setups={}, index_path='./index/index.faiss')\n",
              "  (retriever): FAISSRetriever(\n",
              "    top_k=2, metric=prob, dimensions=1024, total_documents=10\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-large', 'dimensions': 1024, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "  )\n",
              "  (retriever_output_processors): RetrieverOutputToContextStr(deduplicate=True)\n",
              "  (generator): Generator(\n",
              "    model_kwargs={'model': 'gpt-4o-mini', 'temperature': 0.1, 'stream': False}, trainable_prompt_kwargs=[]\n",
              "    (prompt): template: <START_OF_SYSTEM_PROMPT>\n",
              "    {# task desc #}\n",
              "    {% if task_desc_str %}\n",
              "    {{task_desc_str}}\n",
              "    {% else %}\n",
              "    You are a helpful assistant.\n",
              "    {% endif %}\n",
              "    {#input format#}\n",
              "    {% if input_format_str %}\n",
              "    <INPUT_FORMAT>\n",
              "    {{input_format_str}}\n",
              "    </INPUT_FORMAT>\n",
              "    {% endif %}\n",
              "    {# output format #}\n",
              "    {% if output_format_str %}\n",
              "    <OUTPUT_FORMAT>\n",
              "    {{output_format_str}}\n",
              "    </OUTPUT_FORMAT>\n",
              "    {% endif %}\n",
              "    {# tools #}\n",
              "    {% if tools_str %}\n",
              "    <TOOLS>\n",
              "    {{tools_str}}\n",
              "    </TOOLS>\n",
              "    {% endif %}\n",
              "    {# example #}\n",
              "    {% if examples_str %}\n",
              "    <EXAMPLES>\n",
              "    {{examples_str}}\n",
              "    </EXAMPLES>\n",
              "    {% endif %}\n",
              "    {# chat history #}\n",
              "    {% if chat_history_str %}\n",
              "    <CHAT_HISTORY>\n",
              "    {{chat_history_str}}\n",
              "    </CHAT_HISTORY>\n",
              "    {% endif %}\n",
              "    {#contex#}\n",
              "    {% if context_str %}\n",
              "    <CONTEXT>\n",
              "    {{context_str}}\n",
              "    </CONTEXT>\n",
              "    {% endif %}\n",
              "    <END_OF_SYSTEM_PROMPT>\n",
              "    <START_OF_USER_PROMPT>\n",
              "    {% if input_str %}\n",
              "    {{input_str}}\n",
              "    {% endif %}\n",
              "    <END_OF_USER_PROMPT>\n",
              "    {# steps #}\n",
              "    {% if steps_str %}\n",
              "    <START_OF_ASSISTANT_STEPS>\n",
              "    {{steps_str}}\n",
              "    <END_OF_ASSISTANT_STEPS>\n",
              "    {% endif %}\n",
              "    , prompt_kwargs: {'task_desc_str': '\\nYou are a helpful assistant.\\n\\nYour task is to answer the query that may or may not come with context information.\\nWhen context is provided, you should stick to the context and less on your prior knowledge to answer the query.\\n\\nOutput JSON format:\\n{\\n    \"answer\": \"The answer to the query\",\\n}'}, prompt_variables: ['context_str', 'examples_str', 'input_str', 'tools_str', 'chat_history_str', 'steps_str', 'output_format_str', 'task_desc_str', 'input_format_str']\n",
              "    (model_client): OpenAIClient()\n",
              "    (output_processors): JsonParser()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing and Results\n",
        "\n",
        "Let's test our RAG system with a specific question about the research paper:"
      ],
      "metadata": {
        "id": "CK0ded0X-jXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test query about watermarking protein generative models\n",
        "query = \"What are the benefits of watermarking protein generative models?\"\n",
        "\n",
        "print(f\"Question: {query}\\n\")\n",
        "print(\"Processing query through RAG pipeline...\\n\")\n",
        "\n",
        "# Run the complete RAG pipeline\n",
        "response, retrieved_documents = rag.call(query)\n",
        "\n",
        "# Extract and display the answer\n",
        "answer = response.to_dict()['data']['answer']\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\"*60)\n",
        "print(answer)\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "N2Hr2w1Lcgq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e694a7f-d090-47b3-fa49-e4523e1171db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the benefits of watermarking protein generative models?\n",
            "\n",
            "Processing query through RAG pipeline...\n",
            "\n",
            "retrieved_documents: \n",
            " [RetrieverOutput(id=None, doc_indices=[1, 0], doc_scores=[0.5860000252723694, 0.5860000252723694], query='What are the benefits of watermarking protein generative models?', documents=[Document(id=da8d6901-3ebc-4751-839e-87ec1fc15598, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=fb77bee6-1585-4ca5-900e-fd89c826990b, order=0, score=None), Document(id=98aac68c-9cba-46a8-b60b-7f063b2df3d6, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=830f3afb-2243-4256-a377-5c98ef000042, order=0, score=None)])]\n",
            "\n",
            "context_str: \n",
            "  meta_data meta_data\n",
            "\n",
            "============================================================\n",
            "FINAL ANSWER:\n",
            "============================================================\n",
            "The benefits of watermarking protein generative models include: 1) Ensuring the authenticity of generated proteins, 2) Protecting intellectual property by marking proprietary models, 3) Enabling traceability of generated data back to the source model, 4) Preventing misuse of the models by identifying unauthorized use, and 5) Facilitating accountability in research and applications involving generated proteins.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.to_dict()['data']['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3Q_vKtFpHb0s",
        "outputId": "f71801b5-a93f-4e49-cd32-3668455124a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The benefits of watermarking protein generative models include: 1) Ensuring the authenticity of generated proteins, 2) Protecting intellectual property by marking proprietary models, 3) Enabling traceability of generated data back to the source model, 4) Preventing misuse of the models by identifying unauthorized use, and 5) Facilitating accountability in research and applications involving generated proteins.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_documents[0].documents"
      ],
      "metadata": {
        "id": "mYS4aUndklVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc4e232-e077-49fd-a84a-e7d9a7c2fccc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id=da8d6901-3ebc-4751-839e-87ec1fc15598, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=fb77bee6-1585-4ca5-900e-fd89c826990b, order=0, score=None),\n",
              " Document(id=98aac68c-9cba-46a8-b60b-7f063b2df3d6, text='meta_data', meta_data=text, vector='len: 1024', parent_doc_id=830f3afb-2243-4256-a377-5c98ef000042, order=0, score=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Features and Benefits\n",
        "\n",
        "### What Makes This RAG System Special:\n",
        "\n",
        "1. **Persistent Storage**: The database saves both documents and transformers, ensuring consistency across sessions\n",
        "2. **Batch Processing**: Efficient embedding generation through batching\n",
        "3. **Modular Architecture**: Clear separation between data processing and query handling\n",
        "4. **Production-Ready**: Designed to mirror real-world deployment scenarios\n",
        "5. **Flexible Configuration**: Centralized config management for easy tuning\n",
        "\n",
        "### Performance Considerations:\n",
        "\n",
        "- **Chunk Overlap**: The 100-word overlap ensures context preservation across chunks\n",
        "- **Top-K Retrieval**: Limited to 2 documents to focus on most relevant information\n",
        "- **Low Temperature**: Ensures consistent, factual responses\n",
        "- **Deduplication**: Prevents redundant context in the final prompt"
      ],
      "metadata": {
        "id": "qpQ4x97C-27Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LvFQmGmQ-3aT"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}