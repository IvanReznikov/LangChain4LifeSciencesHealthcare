{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1bXSuZZMUjI6IqWfPD-CO2BCt4NzzjROR",
     "timestamp": 1742132988584
    },
    {
     "file_id": "1aMa3Ull4rTcsBqtZwIf6LdVV05YgFMp9",
     "timestamp": 1710751834111
    },
    {
     "file_id": "1F-IiJViaB3Q43_5MTIRj3DlFVce0GAKc",
     "timestamp": 1708151895412
    },
    {
     "file_id": "1chx8G66eko5DnUE2I7gBEbMQW3fYZB_I",
     "timestamp": 1690125116486
    },
    {
     "file_id": "1ppLbg4JfAd16dbszWAhzf1eKZG1q7-BY",
     "timestamp": 1689510633156
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Code to Chapter 3 of LangChain for Life Science and Healthcare book, by Dr. Ivan Reznikov\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/19Jz1nELzpxwBH1OoDnLbAQZakD33jsFx?usp=sharing)"
   ],
   "metadata": {
    "id": "NkTGPUqzzBUK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook demonstrates the core components of LangChain framework, specifically focusing on applications in life sciences and healthcare. We'll explore prompt templates, output parsers, chains, and runnable components that form the building blocks of LangChain applications."
   ],
   "metadata": {
    "id": "XNYDHQv5zoZj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Package Installation and Setup"
   ],
   "metadata": {
    "id": "rRhkZmn_zr5D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#install all proper packages\n",
    "#!pip install -qU langchain langchain-community langchain-core langchain-openai openai tiktoken chromadb pandas pypdf xmltodict\n",
    "!pip install -qU langchain langchain_huggingface langchain-community langchain-core langchain-openai langchain-text-splitters \\\n",
    "  openai tiktoken chromadb pypdf xmltodict transformers"
   ],
   "metadata": {
    "id": "KCTmQXihq9Jk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203103317,
     "user_tz": -240,
     "elapsed": 53041,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d6d96382-8225-43a6-ae23-9993422dec42"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.0/755.0 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip freeze | grep \"lang\\|openai\\|tiktoken|\\chroma|\\transformers\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iuUP6ThU_rqm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203105315,
     "user_tz": -240,
     "elapsed": 1993,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "80f3a157-529c-4d3c-ab24-67aef27b52d4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "google-ai-generativelanguage==0.6.15\n",
      "google-cloud-language==2.17.2\n",
      "langchain==0.3.26\n",
      "langchain-community==0.3.26\n",
      "langchain-core==0.3.66\n",
      "langchain-huggingface==0.3.0\n",
      "langchain-openai==0.3.27\n",
      "langchain-text-splitters==0.3.8\n",
      "langcodes==3.5.0\n",
      "langsmith==0.4.1\n",
      "language_data==1.3.0\n",
      "libclang==18.1.1\n",
      "openai==1.93.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "\n",
    "# Set OpenAI API key from Google Colab's user environment or default\n",
    "def set_openai_api_key(default_key: str = \"YOUR_API_KEY\") -> None:\n",
    "    \"\"\"Set the OpenAI API key from Google Colab's user environment or use a default value.\"\"\"\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"LC4LS_OPENAI_API_KEY\") or default_key\n",
    "\n",
    "\n",
    "try:\n",
    "    set_openai_api_key()\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "metadata": {
    "id": "6-VLK6Vi3_5v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Structured Output Parsing with Pydantic"
   ],
   "metadata": {
    "id": "avPyY_KJzwsH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ],
   "metadata": {
    "id": "zh_Cx5AAOH3f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the patient data structure\n",
    "class PatientAssessment(BaseModel):\n",
    "    diagnosis: str = Field(description=\"Primary medical diagnosis\")\n",
    "    pain_level: int = Field(description=\"Pain level on scale of 0-10\")\n",
    "    symptoms: List[str] = Field(description=\"List of reported symptoms\")\n",
    "    requires_hospitalization: bool = Field(\n",
    "        description=\"Whether patient needs to be hospitalized\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the parser\n",
    "parser = PydanticOutputParser(pydantic_object=PatientAssessment)"
   ],
   "metadata": {
    "id": "WwJShI-80RuV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstrates **structured output parsing** - a crucial feature for healthcare applications:\n",
    "\n",
    "1. **Pydantic Model Definition**:\n",
    "   - `PatientAssessment` class defines the expected structure of medical assessments\n",
    "   - Each field has type hints and descriptions for clarity\n",
    "   - `Field()` provides metadata that helps the LLM understand what's expected\n",
    "\n",
    "2. **PydanticOutputParser**:\n",
    "   - Automatically generates format instructions for the LLM\n",
    "   - Parses the LLM's response into a structured Python object\n",
    "   - Validates data types and structure\n",
    "\n",
    "3. **Prompt Template**:\n",
    "   - `{patient_info}` is the input variable for patient data\n",
    "   - `{format_instructions}` is automatically populated by the parser\n",
    "   - The parser tells the LLM exactly how to format its response\n",
    "\n",
    "4. **Chain Construction**:\n",
    "   - Uses the `|` operator to create a pipeline: prompt → LLM → parser\n",
    "   - Each component transforms the data before passing to the next\n",
    "\n",
    "5. **Type Safety**:\n",
    "   - The result is a fully typed Python object\n",
    "   - No need for manual JSON parsing or type conversion\n",
    "   - Perfect for integration with healthcare systems that require structured data\n",
    "\n",
    "This approach is essential in both life sciences and healthcare where data consistency and validation are critical."
   ],
   "metadata": {
    "id": "WwyapWt_z1sw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the prompt template\n",
    "template = \"\"\"\n",
    "Based on the following patient information, provide a medical assessment:\n",
    "Patient Information: {patient_info}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"patient_info\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ],
   "metadata": {
    "id": "5GnaE19_0ed8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up the LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | llm | parser"
   ],
   "metadata": {
    "id": "MiKMmBaf0ea9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the chain\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"patient_info\": \"45-year-old male presenting with chest pain, shortness of breath, and fever of 101°F for the past 2 days. History of hypertension.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Diagnosis: {result.diagnosis}\")\n",
    "print(f\"Pain Level: {result.pain_level} (Type: {type(result.pain_level)})\")\n",
    "print(f\"Symptoms: {result.symptoms} (Type: {type(result.symptoms)})\")\n",
    "print(\n",
    "    f\"Requires Hospitalization: {result.requires_hospitalization} (Type: {type(result.requires_hospitalization)})\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnC6DRJu0iDA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203159376,
     "user_tz": -240,
     "elapsed": 2194,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "305190b1-aff8-4569-e8ce-0574ed4623f5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Diagnosis: Possible acute coronary syndrome or pneumonia\n",
      "Pain Level: 7 (Type: <class 'int'>)\n",
      "Symptoms: ['chest pain', 'shortness of breath', 'fever of 101°F'] (Type: <class 'list'>)\n",
      "Requires Hospitalization: True (Type: <class 'bool'>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "result, dict(result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oh7SszkO4pLt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203159386,
     "user_tz": -240,
     "elapsed": 8,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "663a7d89-4d1f-42c0-a9fb-d76b02bc43c0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(PatientAssessment(diagnosis='Possible acute coronary syndrome or pneumonia', pain_level=7, symptoms=['chest pain', 'shortness of breath', 'fever of 101°F'], requires_hospitalization=True),\n",
       " {'diagnosis': 'Possible acute coronary syndrome or pneumonia',\n",
       "  'pain_level': 7,\n",
       "  'symptoms': ['chest pain', 'shortness of breath', 'fever of 101°F'],\n",
       "  'requires_hospitalization': True})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hugging Face Model Integration"
   ],
   "metadata": {
    "id": "PxhcDjiI0J-v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"GT4SD/multitask-text-and-chemistry-t5-base-augm\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create a pipeline with the model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    num_beams=5,\n",
    ")\n",
    "\n",
    "# Create a LangChain HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ],
   "metadata": {
    "id": "tjj9L2LMOH1R",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203177687,
     "user_tz": -240,
     "elapsed": 18299,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249,
     "referenced_widgets": [
      "bae0af610bce43cc9a652d27b54d540e",
      "344c197bca9147938333a9b092a39994",
      "a322fe175927424db6ada002da4ee71d",
      "c45b52b0a9a543edb290eb1d9b5cde13",
      "0718a1334be44c9ca688746d2f6d245f",
      "da89cde6d8c74cbdb24480ad7443c053",
      "b0e628d1517440c1aeb1cb19bae1be31",
      "b06094455b3f4a369f885c22eb37a77d",
      "3f8d07925f704c0a8f05515739854157",
      "842b2825328b4539b9be1e3cece96958",
      "015df5b1b80e487e900db388698c1d95",
      "33a8d1ee8b104443b5d0ee969e99d9d9",
      "479382cf9afa46a5a3fbc006fe341c2b",
      "7c5534e2e16940c3b941d829ccbef654",
      "4433aa4e32c74fecab55828fe2d5c18c",
      "f8596f35c4b34e0f971314bde4f02dbf",
      "2b17e1f1f0dc414cad7d8198bda5d1d5",
      "99d2fa826b7f41ffbd7e024487460e46",
      "a442314b43064ae8b176cc80103ba492",
      "9b14ec36b3b04866be71da5324f1c205",
      "9bf578c0d0384231a7c89b34d5a34261",
      "75217a984a184a4391f8c97e33d545df",
      "dfccfc3a532b419188f3fabfdc478718",
      "47da21f80d6444ea952161d65336fb84",
      "50d6ebf2181b4dffb5710353b86734b1",
      "782f7ba5d52b4077b8cf93eaabca72e7",
      "315f51bc94274a62930063654bfaf5e5",
      "bca8ba0b01344d2a916db64bb68ba90b",
      "e5cc9d59009d4fd5aa39a29dca19a27e",
      "1d398d01d7a94bddad214ca9469fe00b",
      "20859f5c195141c8a36ba9f84b1c55d1",
      "d29ec107ab5f4add909c8ccdb4e79d37",
      "8ae75116bbfd41cfaefd588c9a5307f0",
      "50bca2134f0443378052bdfc1cdd83de",
      "da640d4c88bd4d4ab447dc969ef3468f",
      "5ab69b7c84124ebc89fc493d80b27752",
      "628f60d537dc482ebd0b15f5c4bf80fc",
      "1888a24492d9430db7b866a985346a91",
      "d01cbe7ee1d54bd39eae186364517450",
      "4a13c6f490cf4d01891f8e6af97f178e",
      "6966ba820a3843ada173c94a0a0e2bd7",
      "b39bac7e8a5f47feaa9a3b19aba03ab5",
      "481ad6b5af624923957f502b2335ea23",
      "6708f6229d5045f684fe9facf98f3c97",
      "247ef7276e1948bca8cb42bf73477e0f",
      "8f5e3bab25a94154bcdaf28a6421b0cb",
      "e54167e567eb4d8ab2009048643cece2",
      "8bd4f84e37554a3fa483280d72e9ae06",
      "cdf5297bcc594184bf772613658b647b",
      "4cca0684b0ac43f580bb30f89de5fa63",
      "e7898371696640f4a0c4d59260ce83d7",
      "abf207498bf94f49b083c8db3d046b9a",
      "dcd240ffa7a3410691e27decea332a17",
      "b5cfd540bdd04be495265624bf6922aa",
      "18171f13cf4d4cd0afbb455b7b6a6c2e"
     ]
    },
    "outputId": "f4017792-db34-4bd9-f3fe-89c55ba286da"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bae0af610bce43cc9a652d27b54d540e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33a8d1ee8b104443b5d0ee969e99d9d9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfccfc3a532b419188f3fabfdc478718"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50bca2134f0443378052bdfc1cdd83de"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "247ef7276e1948bca8cb42bf73477e0f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstrates **local model integration** using Hugging Face, which is crucial for healthcare applications with privacy concerns:\n",
    "\n",
    "1. **Specialized Chemistry Model**:\n",
    "   - `GT4SD/multitask-text-and-chemistry-t5-base-augm` is a T5 model trained on chemistry and text tasks\n",
    "   - Perfect for pharmaceutical and chemical research applications\n",
    "   - Runs completely offline once downloaded\n",
    "\n",
    "2. **Pipeline Configuration**:\n",
    "   - `temperature=0`: Ensures deterministic, reproducible outputs (critical in healthcare)\n",
    "   - `max_length=512`: Limits response length\n",
    "   - `num_beams=5`: Uses beam search for higher quality outputs\n",
    "\n",
    "3. **LangChain Integration**:\n",
    "   - `HuggingFacePipeline` wraps the Hugging Face pipeline\n",
    "   - Makes it compatible with LangChain's chain architecture\n",
    "   - Enables consistent interface across different model providers\n",
    "\n",
    "4. **Prompt Engineering Comparison**:\n",
    "   - **Basic prompt**: Minimal context, relies on model's training\n",
    "   - **Prompt 1**: Adds role context (\"as a chemist\")\n",
    "   - **Prompt 2**: More elaborate role-playing with professional context\n",
    "   - Demonstrates how prompt engineering affects model behavior\n",
    "\n",
    "5. **Chain Architecture**:\n",
    "   - Each chain follows the same pattern: prompt → model → output parser\n",
    "   - `StrOutputParser()` extracts the text from the model's response\n",
    "   - Shows how different prompts can be easily tested with the same underlying model\n",
    "\n",
    "This approach is valuable for healthcare organizations that need to keep sensitive data on-premises while still leveraging AI capabilities."
   ],
   "metadata": {
    "id": "bP2bUj3s0bTy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define prompt templates\n",
    "basic_prompt = PromptTemplate.from_template(\"{input_text}\")\n",
    "prompt1 = PromptTemplate.from_template(\n",
    "    \"Continue the following phrase as a chemist: {input_text}\"\n",
    ")\n",
    "prompt2 = PromptTemplate.from_template(\n",
    "    \"You are a professional chemistry researcher.\\nFinish the following sentence: {input_text}\"\n",
    ")"
   ],
   "metadata": {
    "id": "-Aqno9b9OMce"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create chains for each prompt\n",
    "basic_chain = basic_prompt | llm | StrOutputParser()\n",
    "prompt1_chain = prompt1 | llm | StrOutputParser()\n",
    "prompt2_chain = prompt2 | llm | StrOutputParser()"
   ],
   "metadata": {
    "id": "EnExlajbOODd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# The text to be continued\n",
    "TEXT = \"The formula of dihydrogen monoxide is\"\n",
    "\n",
    "# Run the chains\n",
    "basic_result = basic_chain.invoke({\"input_text\": TEXT})\n",
    "prompt1_result = prompt1_chain.invoke({\"input_text\": TEXT})\n",
    "prompt2_result = prompt2_chain.invoke({\"input_text\": TEXT})"
   ],
   "metadata": {
    "id": "jV0nV8RAM0BH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203204044,
     "user_tz": -240,
     "elapsed": 26343,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "75e6b669-e4db-4972-ee68-2234e5c3b260"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Basic prompt result:\", basic_result)\n",
    "print(\"Prompt 1 result:\", prompt1_result)\n",
    "print(\"Prompt 2 result:\", prompt2_result)"
   ],
   "metadata": {
    "id": "jeXiaoOGOQ0q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203204099,
     "user_tz": -240,
     "elapsed": 61,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9cb0889a-d0a9-4c66-b037-4bec5b4e6e59"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Basic prompt result: [O-][Mn](=O)(=O)=O\n",
      "Prompt 1 result: [O-][Mn](=O)(=O)=O.[O-][Mn](=O)(=O)=O.[O-][Mn](=O)(=O)=O\n",
      "Prompt 2 result: The molecule is a dihydrogen monoxide. It is a conjugate base of a dihydrogen monoxide(2+). It is a conjugate acid of a dihydrogen monoxide(1-).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see above, prompts matter!"
   ],
   "metadata": {
    "id": "N5ot5LhXM0bL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Few-Shot Learning for Medical Reasoning"
   ],
   "metadata": {
    "id": "Gy3CtmR20jhW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Is Penicillin effective against E. coli?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow-up questions needed here: Yes.\n",
    "Follow up: What class of antibiotics does Penicillin belong to?\n",
    "Intermediate answer: Penicillin belongs to the beta-lactam class of antibiotics.\n",
    "Follow up: What mechanism of action does Penicillin have?\n",
    "Intermediate answer: Penicillin interferes with the synthesis of the bacterial\n",
    "cell wall.\n",
    "Follow up: Is E. coli resistant to beta-lactam antibiotics?\n",
    "Intermediate answer: Many strains of E. coli have developed resistance to\n",
    "beta-lactam antibiotics, including Penicillin.\n",
    "So the final answer is: Penicillin is generally not effective against E. coli due\n",
    "to resistance.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Do Aspirin and Ibuprofen have the same mechanism of action?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow-up questions needed here: Yes.\n",
    "Follow up: What is the mechanism of action of aspirin?\n",
    "Intermediate Answer: Aspirin works by inhibiting the enzyme cyclooxygenase (COX),\n",
    "which reduces the formation of prostaglandins and thromboxanes, leading to its\n",
    "anti-inflammatory and anticoagulant effects.\n",
    "Follow up: What is the mechanism of action of Ibuprofen?\n",
    "Intermediate Answer: Ibuprofen also inhibits the cyclooxygenase (COX) enzyme,\n",
    "reducing the production of prostaglandins.\n",
    "So the final answer is: Yes, both Aspirin and Ibuprofen have the same mechanism of\n",
    "action, which is the inhibition of the COX enzyme.\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ],
   "metadata": {
    "id": "XCQBk_wm264k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstrates **few-shot learning** with **chain-of-thought reasoning** - essential for complex medical reasoning:\n",
    "\n",
    "1. **Few-Shot Learning Concept**:\n",
    "   - Provides the model with examples of the desired reasoning pattern\n",
    "   - Teaches the model to break down complex medical questions systematically\n",
    "   - More effective than zero-shot prompting for specialized domains\n",
    "\n",
    "2. **Chain-of-Thought Reasoning**:\n",
    "   - Each example shows step-by-step logical progression\n",
    "   - \"Follow up questions\" break complex problems into manageable parts\n",
    "   - \"Intermediate answers\" provide reasoning transparency\n",
    "   - Critical for medical applications where reasoning must be auditable\n",
    "\n",
    "3. **Medical Domain Examples**:\n",
    "   - **Example 1**: Antibiotic resistance reasoning\n",
    "   - **Example 2**: Drug mechanism comparison\n",
    "   - Both examples demonstrate multi-step medical reasoning\n",
    "\n",
    "4. **Template Structure**:\n",
    "   - `example_prompt`: Formats each individual example\n",
    "   - `FewShotPromptTemplate`: Combines all examples with the new question\n",
    "   - `suffix`: Adds the new question in the same format\n",
    "\n",
    "5. **Comparison Study**:\n",
    "   - First call: Direct LLM invocation without examples\n",
    "   - Second call: Using the few-shot chain\n",
    "   - Shows how examples improve the quality and structure of medical reasoning\n",
    "\n",
    "6. **Complex Medical Question**:\n",
    "   - Cancer genetics question requires multi-layered understanding\n",
    "   - Tests the model's ability to apply the learned reasoning pattern\n",
    "   - Demonstrates how few-shot learning scales to complex scenarios"
   ],
   "metadata": {
    "id": "jA2fWCr70of5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# setting the prompt template to process the examples list\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "# setting the prompt template to be used with the LLM\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ],
   "metadata": {
    "id": "Gi7XbLuV2-f3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TEXT = \"How do genetic mutations in oncogenes and tumor suppressor genes interact to drive the progression of metastatic cancer?\""
   ],
   "metadata": {
    "id": "jfzGNHCL3hWc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")"
   ],
   "metadata": {
    "id": "tv2df28U3z0K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "llm.invoke(TEXT).content"
   ],
   "metadata": {
    "id": "MyAxiORR3GEj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203214387,
     "user_tz": -240,
     "elapsed": 10261,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "outputId": "cf76a9fd-24b9-47e4-fa7c-31fe1b37f19d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Genetic mutations in oncogenes and tumor suppressor genes play crucial roles in the development and progression of metastatic cancer. Their interactions can create a complex network of signaling pathways that drive tumorigenesis, invasion, and metastasis. Here’s how these mutations interact and contribute to cancer progression:\\n\\n### Oncogenes\\nOncogenes are mutated forms of normal genes (proto-oncogenes) that promote cell growth and division. When mutated, they can lead to uncontrolled cell proliferation. Common mechanisms of oncogene activation include point mutations, gene amplifications, and chromosomal rearrangements. Examples of oncogenes include:\\n\\n- **KRAS**: Mutations in KRAS lead to continuous activation of signaling pathways that promote cell growth and survival.\\n- **MYC**: Overexpression of MYC can drive cell proliferation and metabolism.\\n\\n### Tumor Suppressor Genes\\nTumor suppressor genes normally function to inhibit cell growth, repair DNA damage, or induce apoptosis. Mutations in these genes can lead to loss of function, allowing cells to escape growth control. Key tumor suppressor genes include:\\n\\n- **TP53**: Mutations in TP53 impair the cell's ability to respond to DNA damage, leading to genomic instability.\\n- **RB1**: Loss of RB1 function removes a critical checkpoint in the cell cycle, promoting uncontrolled cell division.\\n\\n### Interaction Between Oncogenes and Tumor Suppressor Genes\\n1. **Cooperation in Tumorigenesis**: The activation of oncogenes often requires the inactivation of tumor suppressor genes. For example, a cell may acquire a mutation in an oncogene that drives proliferation, but this effect is often amplified by the concurrent loss of a tumor suppressor gene that would normally inhibit this growth.\\n\\n2. **Genomic Instability**: Mutations in tumor suppressor genes like TP53 can lead to genomic instability, which increases the likelihood of further mutations in oncogenes and other tumor suppressor genes. This accumulation of mutations can drive the progression from localized tumors to metastatic disease.\\n\\n3. **Altered Signaling Pathways**: Oncogene activation can rewire cellular signaling pathways, leading to changes in the tumor microenvironment. For instance, activated oncogenes can promote angiogenesis (formation of new blood vessels) and enhance the ability of cancer cells to invade surrounding tissues. Tumor suppressor gene loss can further exacerbate these changes by removing regulatory controls.\\n\\n4. **Evasion of Apoptosis**: Oncogenes can promote survival pathways that help cancer cells evade apoptosis, while mutations in tumor suppressor genes can further diminish the apoptotic response. This combination allows cancer cells to survive and proliferate even in unfavorable conditions.\\n\\n5. **Metastatic Potential**: The interplay between oncogenes and tumor suppressor genes can enhance the metastatic potential of cancer cells. For example, mutations that activate epithelial-mesenchymal transition (EMT) pathways (often driven by oncogenes) can be facilitated by the loss of tumor suppressor genes, allowing cancer cells to detach, invade, and migrate to distant sites.\\n\\n### Conclusion\\nThe progression of metastatic cancer is a multifaceted process driven by the interplay of oncogenes and tumor suppressor genes. Their mutations create a permissive environment for tumor growth, invasion, and metastasis through mechanisms such as enhanced proliferation, evasion of apoptosis, and increased genomic instability. Understanding these interactions is crucial for developing targeted therapies and improving cancer treatment outcomes.\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "basic_chain = prompt | llm | StrOutputParser()\n",
    "basic_chain.invoke({\"input\": TEXT})"
   ],
   "metadata": {
    "id": "UB-Nj5713EIV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203219838,
     "user_tz": -240,
     "elapsed": 5450,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "outputId": "40051a89-cc94-49c2-c295-3787f2a371f8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Are follow-up questions needed here: Yes.  \\nFollow up: What are oncogenes?  \\nIntermediate Answer: Oncogenes are mutated forms of normal genes (proto-oncogenes) that promote cell growth and division. When mutated, they can lead to uncontrolled cell proliferation.  \\nFollow up: What are tumor suppressor genes?  \\nIntermediate Answer: Tumor suppressor genes are genes that normally help regulate cell growth and division, repair DNA, and promote apoptosis (programmed cell death). When these genes are mutated or inactivated, they lose their ability to control cell growth.  \\nFollow up: How do mutations in oncogenes contribute to cancer progression?  \\nIntermediate Answer: Mutations in oncogenes can lead to the overactivation of signaling pathways that promote cell proliferation and survival, contributing to tumor growth and metastasis.  \\nFollow up: How do mutations in tumor suppressor genes contribute to cancer progression?  \\nIntermediate Answer: Mutations in tumor suppressor genes can result in the loss of growth control, allowing cells to divide uncontrollably and evade apoptosis, which can lead to tumor development and metastasis.  \\nFollow up: How do these mutations interact in the context of metastatic cancer?  \\nIntermediate Answer: The interaction between mutated oncogenes and inactivated tumor suppressor genes creates a cellular environment that favors uncontrolled growth, survival, and the ability to invade other tissues, which are key characteristics of metastatic cancer.  \\n\\nSo the final answer is: Genetic mutations in oncogenes promote uncontrolled cell growth, while mutations in tumor suppressor genes lead to the loss of growth regulation. Together, these mutations interact to drive the progression of metastatic cancer by creating a cellular environment conducive to tumor growth and invasion.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Runnable Components and Composition"
   ],
   "metadata": {
    "id": "ihJBZfXe0wzF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# A RunnableSequence constructed using the `|` operator\n",
    "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)"
   ],
   "metadata": {
    "id": "Cg-Jey18M3W-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section introduces **LangChain's Runnable interface** - the foundation for building complex AI workflows:\n",
    "\n",
    "1. **RunnableLambda**:\n",
    "   - Wraps simple Python functions into LangChain-compatible components\n",
    "   - Enables functional programming patterns in AI pipelines\n",
    "   - Provides consistent interfaces for invoke, batch, and stream operations\n",
    "\n",
    "2. **Sequential Composition**:\n",
    "   - `|` operator chains operations left-to-right\n",
    "   - Output of first operation becomes input of second\n",
    "   - Similar to Unix pipes but for AI workflows\n",
    "\n",
    "3. **Parallel Composition**:\n",
    "   - Dictionary syntax creates parallel branches\n",
    "   - Single input is processed by multiple functions simultaneously\n",
    "   - Results are combined into a dictionary with named outputs\n",
    "\n",
    "4. **Batch Processing**:\n",
    "   - `batch()` method processes multiple inputs efficiently\n",
    "   - Maintains the same transformation logic across all inputs\n",
    "   - Essential for processing large datasets in healthcare applications\n",
    "\n",
    "5. **Flexibility**:\n",
    "   - Can combine sequential and parallel operations\n",
    "   - Building blocks for complex medical data processing pipelines\n",
    "   - Enables sophisticated workflows while maintaining readability\n",
    "\n",
    "This pattern is foundational for building scalable healthcare AI systems where data needs to flow through multiple processing steps."
   ],
   "metadata": {
    "id": "fBNwfHRF05I4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sequence.invoke(1)  # 4"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKwiEojIN4m8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203219875,
     "user_tz": -240,
     "elapsed": 24,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "4dc0f801-034e-4b30-d54a-75558add9f42"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sequence.batch([1, 2, 3])  # [4, 6, 8]"
   ],
   "metadata": {
    "id": "k-FjG01tN_CO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203219893,
     "user_tz": -240,
     "elapsed": 23,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "a942f8af-1f36-41c0-df47-5ec12c004747",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[4, 6, 8]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# A sequence that contains a RunnableParallel constructed using a dict literal\n",
    "sequence = RunnableLambda(lambda x: x + 1) | {\n",
    "    \"mul_2\": RunnableLambda(lambda x: x * 2),\n",
    "    \"mul_5\": RunnableLambda(lambda x: x * 5),\n",
    "}"
   ],
   "metadata": {
    "id": "RsBXi5ztN46h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sequence.invoke(1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8dvRYemM7V5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203219933,
     "user_tz": -240,
     "elapsed": 28,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "43c28a90-69b3-40d0-e3fe-05db3597f69b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mul_2': 4, 'mul_5': 10}"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sequence.batch([1, 2, 3])  # [4, 6, 8]"
   ],
   "metadata": {
    "id": "fwCla3vMN98o",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203219957,
     "user_tz": -240,
     "elapsed": 20,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "a9ea6022-1f7f-4d6b-f735-b5950ab088ad",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'mul_2': 4, 'mul_5': 10},\n",
       " {'mul_2': 6, 'mul_5': 15},\n",
       " {'mul_2': 8, 'mul_5': 20}]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Practical Example: Temperature Conversion with LangChain"
   ],
   "metadata": {
    "id": "3Hhrsc961P2k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# Define temperature conversion functions\n",
    "def kelvin_to_celsius(kelvin_temp):\n",
    "    \"\"\"Convert Kelvin to Celsius\"\"\"\n",
    "    return kelvin_temp - 273.15\n",
    "\n",
    "\n",
    "def celsius_to_fahrenheit(celsius_temp):\n",
    "    \"\"\"Convert Celsius to Fahrenheit\"\"\"\n",
    "    return (celsius_temp * 9 / 5) + 32\n",
    "\n",
    "\n",
    "def format_temperature_result(temp_dict):\n",
    "    \"\"\"Format the temperature conversion results\"\"\"\n",
    "    return {\n",
    "        \"original_kelvin\": temp_dict[\"kelvin\"],\n",
    "        \"celsius\": round(temp_dict[\"celsius\"], 2),\n",
    "        \"fahrenheit\": round(temp_dict[\"fahrenheit\"], 2),\n",
    "        \"formatted_output\": f\"{temp_dict['kelvin']}K = {round(temp_dict['celsius'], 2)}°C = {round(temp_dict['fahrenheit'], 2)}°F\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Create RunnableLambda components\n",
    "kelvin_to_celsius_runnable = RunnableLambda(kelvin_to_celsius)\n",
    "celsius_to_fahrenheit_runnable = RunnableLambda(celsius_to_fahrenheit)\n",
    "format_result_runnable = RunnableLambda(format_temperature_result)"
   ],
   "metadata": {
    "id": "JD_c5uh31Icr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This comprehensive example demonstrates **practical application of LangChain Runnables** for temperature conversion with medical context:\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - Pure Python functions for temperature conversions\n",
    "   - Follows single responsibility principle\n",
    "   - Easy to test and maintain\n",
    "\n",
    "2. **RunnableLambda Wrapping**:\n",
    "   - Converts Python functions into LangChain components\n",
    "   - Enables composition with other LangChain components\n",
    "   - Provides consistent interface (invoke, batch, stream)\n",
    "\n",
    "3. **Sequential Processing**:\n",
    "   - Demonstrates step-by-step transformation\n",
    "   - Kelvin → Celsius → Fahrenheit pipeline\n",
    "   - Maintains intermediate results for transparency\n",
    "\n",
    "4. **Parallel Processing**:\n",
    "   - Multiple conversions happen simultaneously\n",
    "   - More efficient for independent calculations\n",
    "   - Shows different architectural approaches\n",
    "\n",
    "5. **Data Structure Management**:\n",
    "   - Dictionary-based data flow\n",
    "   - Preserves original values alongside conversions\n",
    "   - Structured output for downstream processing\n",
    "\n",
    "6. **Batch Processing**:\n",
    "   - Efficiently processes multiple temperatures\n",
    "   - Maintains consistent transformation logic\n",
    "   - Essential for processing large datasets\n",
    "\n",
    "7. **Medical Application**:\n",
    "   - Body temperature analysis with clinical context\n",
    "   - Fever detection and categorization\n",
    "   - Demonstrates real-world healthcare application\n",
    "\n",
    "8. **Composition Patterns**:\n",
    "   - Shows how to combine different types of operations\n",
    "   - Parallel and sequential processing in the same pipeline\n",
    "   - Scalable architecture for complex medical workflows"
   ],
   "metadata": {
    "id": "jKJavpD-08pA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Method 1: Sequential conversion (Kelvin → Celsius → Fahrenheit)\n",
    "sequential_conversion = (\n",
    "    RunnableLambda(lambda x: x)  # Pass through the input\n",
    "    | {\n",
    "        \"kelvin\": RunnableLambda(lambda x: x),  # Keep original value\n",
    "        \"celsius\": kelvin_to_celsius_runnable,  # Convert to Celsius\n",
    "    }\n",
    "    | RunnableLambda(\n",
    "        lambda x: {\n",
    "            \"kelvin\": x[\"kelvin\"],\n",
    "            \"celsius\": x[\"celsius\"],\n",
    "            \"fahrenheit\": celsius_to_fahrenheit(\n",
    "                x[\"celsius\"]\n",
    "            ),  # Convert Celsius to Fahrenheit\n",
    "        }\n",
    "    )\n",
    "    | format_result_runnable\n",
    ")\n",
    "\n",
    "\n",
    "# Method 2: Direct conversion functions\n",
    "def kelvin_to_fahrenheit_direct(kelvin_temp):\n",
    "    \"\"\"Direct conversion from Kelvin to Fahrenheit\"\"\"\n",
    "    celsius = kelvin_temp - 273.15\n",
    "    fahrenheit = (celsius * 9 / 5) + 32\n",
    "    return fahrenheit\n",
    "\n",
    "\n",
    "# Create parallel conversion chain\n",
    "parallel_conversion = (\n",
    "    RunnableLambda(lambda x: x)\n",
    "    | {\n",
    "        \"kelvin\": RunnableLambda(lambda x: x),\n",
    "        \"celsius\": kelvin_to_celsius_runnable,\n",
    "        \"fahrenheit\": RunnableLambda(kelvin_to_fahrenheit_direct),\n",
    "    }\n",
    "    | format_result_runnable\n",
    ")"
   ],
   "metadata": {
    "id": "R-QlnD8W1ZYE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test with various temperatures\n",
    "test_temperatures = [\n",
    "    273.15,\n",
    "    300,\n",
    "    310.15,\n",
    "    373.15,\n",
    "]  # 0°C, ~27°C, ~37°C (body temp), 100°C\n",
    "\n",
    "print(\"=== Sequential Conversion Results ===\")\n",
    "for temp in test_temperatures:\n",
    "    result = sequential_conversion.invoke(temp)\n",
    "    print(f\"Sequential: {result['formatted_output']}\")\n",
    "\n",
    "print(\"\\n=== Parallel Conversion Results ===\")\n",
    "for temp in test_temperatures:\n",
    "    result = parallel_conversion.invoke(temp)\n",
    "    print(f\"Parallel: {result['formatted_output']}\")\n",
    "\n",
    "# Batch processing example\n",
    "print(\"\\n=== Batch Processing ===\")\n",
    "batch_results = parallel_conversion.batch(test_temperatures)\n",
    "for result in batch_results:\n",
    "    print(f\"Batch: {result['formatted_output']}\")\n",
    "\n",
    "# Medical context example: Converting body temperature readings\n",
    "print(\"\\n=== Medical Context: Body Temperature Analysis ===\")\n",
    "body_temps_kelvin = [\n",
    "    309.15,\n",
    "    310.15,\n",
    "    311.15,\n",
    "    312.15,\n",
    "]  # Range of body temperatures in Kelvin\n",
    "\n",
    "medical_analysis = RunnableLambda(lambda x: x) | {\n",
    "    \"temperature_data\": parallel_conversion,\n",
    "    \"medical_assessment\": RunnableLambda(\n",
    "        lambda temp_k: {\n",
    "            \"normal_range\": 36.1 <= (temp_k - 273.15) <= 37.2,\n",
    "            \"fever_status\": (\n",
    "                \"Fever\"\n",
    "                if (temp_k - 273.15) > 37.2\n",
    "                else \"Normal\" if (temp_k - 273.15) >= 36.1 else \"Hypothermia\"\n",
    "            ),\n",
    "            \"celsius_value\": round(temp_k - 273.15, 1),\n",
    "        }\n",
    "    ),\n",
    "}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TU39eR7v1UQN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203220048,
     "user_tz": -240,
     "elapsed": 32,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "74107de3-e9be-4f42-ff0f-89bef4224b13"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Sequential Conversion Results ===\n",
      "Sequential: 273.15K = 0.0°C = 32.0°F\n",
      "Sequential: 300K = 26.85°C = 80.33°F\n",
      "Sequential: 310.15K = 37.0°C = 98.6°F\n",
      "Sequential: 373.15K = 100.0°C = 212.0°F\n",
      "\n",
      "=== Parallel Conversion Results ===\n",
      "Parallel: 273.15K = 0.0°C = 32.0°F\n",
      "Parallel: 300K = 26.85°C = 80.33°F\n",
      "Parallel: 310.15K = 37.0°C = 98.6°F\n",
      "Parallel: 373.15K = 100.0°C = 212.0°F\n",
      "\n",
      "=== Batch Processing ===\n",
      "Batch: 273.15K = 0.0°C = 32.0°F\n",
      "Batch: 300K = 26.85°C = 80.33°F\n",
      "Batch: 310.15K = 37.0°C = 98.6°F\n",
      "Batch: 373.15K = 100.0°C = 212.0°F\n",
      "\n",
      "=== Medical Context: Body Temperature Analysis ===\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for temp_k in body_temps_kelvin:\n",
    "    analysis = medical_analysis.invoke(temp_k)\n",
    "    temp_data = analysis[\"temperature_data\"]\n",
    "    medical_data = analysis[\"medical_assessment\"]\n",
    "\n",
    "    print(f\"Temperature: {temp_data['formatted_output']}\")\n",
    "    print(f\"  Medical Status: {medical_data['fever_status']}\")\n",
    "    print(f\"  Normal Range: {medical_data['normal_range']}\")\n",
    "    print()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oxwu4Mwy1c5o",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751203220076,
     "user_tz": -240,
     "elapsed": 26,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     }
    },
    "outputId": "c715c772-a7fc-4f3b-d598-692e4a7cd5b5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Temperature: 309.15K = 36.0°C = 96.8°F\n",
      "  Medical Status: Hypothermia\n",
      "  Normal Range: False\n",
      "\n",
      "Temperature: 310.15K = 37.0°C = 98.6°F\n",
      "  Medical Status: Normal\n",
      "  Normal Range: True\n",
      "\n",
      "Temperature: 311.15K = 38.0°C = 100.4°F\n",
      "  Medical Status: Fever\n",
      "  Normal Range: False\n",
      "\n",
      "Temperature: 312.15K = 39.0°C = 102.2°F\n",
      "  Medical Status: Fever\n",
      "  Normal Range: False\n",
      "\n"
     ]
    }
   ]
  }
 ]
}
